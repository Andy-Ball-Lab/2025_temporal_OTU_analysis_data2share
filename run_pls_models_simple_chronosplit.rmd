---
title: "PLS pipeline"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

# Minimal, PLS pipeline with per-digester time splits

```{r}

# ------------------------------------------------------------
# Minimal, PLS pipeline with per-digester time splits
# ------------------------------------------------------------
# What it does (simply and deterministically):
# 1) Chronological split PER DIGESTER: first 85% -> training, last 15% -> test.
# 2) Chronological tail-end CV/tuning on the training set, per-digester (rolling origin).
# 3) Tune mixOmics PLS (num_comp, predictor_prop) with tidymodels.
# 4) Plot
# 5) Summary table: RMSE, MAPE, R^2 for CV(train) and Test.
#
# Assumes data frame contains:
#   - an ID column for the unit (e.g., "Digester")
#   - a POSIXct/Date time column (e.g., "Sample_Date")
#   - one target/outcome column
#   - many numeric predictor columns (e.g., OTUs or FLASVs)
#
# ------------------------------------------------------------

suppressPackageStartupMessages({
  library(tidyverse)
  library(tidymodels)
  library(plsmod)      # PLS wrapper
  library(mixOmics)    # PLS engine
  library(rsample)     # resampling
  library(lubridate)
  library(rlang)
  library(glue)
  library(ggrepel)
})

# --- Patch for plsmod mixOmics bug ("incorrect number of dimensions")
# see https://github.com/tidymodels/plsmod/issues/47 (unanswered at time of writing this)
fix_multi_numeric_preds <- dget("./custom-functions/fix_multi_numeric_preds.R")
# Register the patch in the main session
assignInNamespace("multi_numeric_preds", fix_multi_numeric_preds, ns = "plsmod")


# ----------------------- USER SETTINGS -----------------------
# Point to your data and choose columns
# If you're reading from RDS:
df <- read.csv("./data/mdata.species.clr_all.csv")
df$Sample_Date   <-  lubridate::as_date(df$Sample_Date)
# For demonstration, we expect 'df' to already exist in the environment.
# If not, uncomment the readRDS above and set the correct path.

id_col   <- "Digester"
time_col <- "Sample_Date"
outcome  <- "A"   # change as needed

# Which columns are predictors?
# If your table uses OTU/FLASV names, this default selects them automatically.
predictor_selector <- function(data) {
  otu <- grep("_otu$|FLASV", names(data), value = TRUE)
  if (length(otu) == 0) {
    # fallback: all numeric predictors except id/time/outcome
    setdiff(names(data)[vapply(data, is.numeric, logical(1))],
            c(id_col, time_col, outcome))
  } else {
    otu
  }
}

# CV settings (on the TRAINING set only)
initial_prop <- 0.70   # first 60% of each digester's training rows start the rolling window
assess_n     <- 4      # assessment block size
n_windows_id <- 5       # number of tail blocks per digester

# Tuning grid size
n_levels <- 5          # modest grid to keep it simple


# --------------------- HELPER FUNCTIONS ----------------------
make_train_test_85_15 <- function(data, id_col, time_col) {
  data %>%
    dplyr::arrange(.data[[id_col]], .data[[time_col]]) %>%
    dplyr::group_by(.data[[id_col]]) %>%
    dplyr::mutate(
      .n   = dplyr::n(),
      .cut = floor(0.85 * .n),
      .set = dplyr::if_else(dplyr::row_number() <= .cut, "train", "test")
    ) %>%
    dplyr::ungroup() %>%
    dplyr::select(-dplyr::all_of(c(".n", ".cut")))
}

# Tail-only CV with contiguous, non-overlapping assessment blocks hugging the end of TRAIN
make_tail_only_cv <- function(train_df, id_col, time_col,
                              initial_prop = 0.6,
                              assess = assess_n,
                              n_windows_per_id = n_windows_id) {

  # Add a global row index (.rid) so we can map per-digester rows back to the full data
    train_df <- train_df %>% dplyr::mutate(.rid = dplyr::row_number())

# Prepare a container for all rsample splits and a running counter
  split_list <- list(); split_id <- 0L

  train_df %>%
    dplyr::arrange(.data[[id_col]], .data[[time_col]]) %>%
    # Ensure rows are ordered by digester then by time
    dplyr::group_split(.data[[id_col]], .keep = TRUE) %>%
    # Split into a list: one tibble per digester (g)
    purrr::walk(function(g) {
      # Iterate over digesters (each 'g' is one digester's training data)
      g <- g %>% dplyr::mutate(.i = dplyr::row_number()) #inside a digester, add a local row index .i (1..n) for clarity
      n_g <- nrow(g)
      if (n_g < 10) return(NULL)
      # bail out if there are fewer than 10 training rows for that digester.
      init_n <- max(1L, floor(initial_prop * n_g))
      # Check if we have enough rows AFTER the initial history to form the full tail
      total_tail <- n_windows_per_id * assess
      if ((n_g - init_n) < total_tail) {
        # not enough tail to make all blocks; try fewer windows
        n_possible <- floor((n_g - init_n) / assess)
        if (n_possible < 1) return(NULL)  # If not even one block fits, skip this digester.
        n_blocks <- n_possible
      } else {
        n_blocks <- n_windows_per_id
        # Otherwise, use the requested number of windows
      }

      # indices for the tail zone
      tail_start <- n_g - (n_blocks * assess) + 1L #tail zone inside the digester’s training series
      tail_idx <- seq.int(tail_start, n_g)
      # Compute the tail zone to be evaluated (contiguous, at the end of TRAIN)

      # build contiguous blocks of size assess, in chronological order
      for (b in seq_len(n_blocks)) {
        block_start <- tail_start + (b - 1L) * assess
        block_end   <- block_start + assess - 1L
        assess_local <- seq.int(block_start, block_end)
        # Define the assessment block (size = 'assess'), contiguous and non-overlapping

        # analysis is all rows BEFORE this block start
        anal_end <- block_start - 1L
        # must also be >= init_n to keep an initial history; enforce that
        anal_end <- max(anal_end, init_n)
        if (anal_end < 1L) next
        # skip if somehow nothing to train on
        analysis_local <- seq.int(1L, anal_end)
        # Analysis is cumulative from the first row up to 'anal_end'

        # map to global row ids
        anal_global <- g$.rid[analysis_local]
        asse_global <- g$.rid[assess_local]
        # Map local indices to global indices so rsample can reference 'train_df'

        split_id  <<- split_id + 1L
        split_list[[split_id]] <<- rsample::make_splits(
          list(analysis = anal_global, assessment = asse_global),
          data = train_df
        )
        # Create and store this rsample split
      }
    })

  rsample::manual_rset(splits = split_list,
                       ids    = sprintf("cv_tail_%03d", seq_along(split_list)))
  # Bundle all splits into a resampling set that tidymodels understands
}

inspect_tail_blocks <- function(train_df, id_col, time_col,
                                initial_prop = 0.6,
                                assess = assess_n,
                                n_windows_per_id = n_windows_id) {
  # Build the same CV object the model uses
  rs <- make_tail_only_cv(train_df, id_col, time_col,
                          initial_prop = initial_prop,
                          assess = assess,
                          n_windows_per_id = n_windows_per_id)

  tibble::tibble(
    id = purrr::map_chr(rs$splits, ~ {
      d <- rsample::analysis(.x)        # a data frame
      as.character(d[[id_col]][1])      # take the digester id from the analysis set
    }),
    assess_rows = purrr::map_chr(rs$splits, ~ {
      a <- rsample::assessment(.x)      # a data frame
      paste0("[", min(a$.rid), "…", max(a$.rid), "]")  # uses the .rid column
    }),
    assess_dates = purrr::map_chr(rs$splits, ~ {
      a <- rsample::assessment(.x)
      paste0("[", min(a[[time_col]]), " … ", max(a[[time_col]]), "]")
    })
  )
}

metric_set_cv  <- metric_set(rmse, mape)      # tune on stable metrics
metric_set_all <- metric_set(rmse, mape, rsq) # report including rsq

# ---------------------- MAIN WORKFLOW ------------------------
stopifnot(all(c(id_col, time_col, outcome) %in% names(df)))

df <- df %>%
  dplyr::arrange(.data[[id_col]], .data[[time_col]]) %>%
  tidyr::drop_na(dplyr::all_of(c(id_col, time_col, outcome)))

pred_cols <- predictor_selector(df)
stopifnot(length(pred_cols) > 0)

df_mod <- df %>%
  dplyr::select(dplyr::all_of(c(id_col, time_col, outcome, pred_cols))) %>%
  dplyr::arrange(.data[[id_col]], .data[[time_col]])

# 85/15 split per digester
df_split <- make_train_test_85_15(df_mod, id_col, time_col)
train_dat <- df_split %>% dplyr::filter(.set == "train") %>% dplyr::select(-.set)
test_dat  <- df_split %>% dplyr::filter(.set == "test")  %>% dplyr::select(-.set)

inspect_tail_blocks(train_dat,
                    id_col = "Digester",
                    time_col = "Sample_Date",
                    initial_prop = 0.6,
                    assess = assess_n,
                    n_windows_per_id = n_windows_id)

# Tail-only CV: exactly up to 6 non-overlapping tail blocks per digester
resamps <- make_tail_only_cv(
  train_df          = train_dat,
  id_col            = id_col,
  time_col          = time_col,
  initial_prop      = initial_prop,
  assess            = assess_n,
  n_windows_per_id  = n_windows_id
)

# Recipe & model
rec <- recipes::recipe(stats::as.formula(glue("`{outcome}` ~ .")), data = train_dat) %>%
  recipes::update_role(dplyr::all_of(id_col),   new_role = "id") %>%
  recipes::update_role(dplyr::all_of(time_col), new_role = "time") %>%
  recipes::step_zv(recipes::all_predictors()) %>%
  recipes::step_normalize(recipes::all_predictors())

pls_spec <- parsnip::pls(mode = "regression",
                         num_comp = tune(),
                         predictor_prop = tune()) %>%
  parsnip::set_engine("mixOmics")

wf <- workflows::workflow() %>%
  workflows::add_recipe(rec) %>%
  workflows::add_model(pls_spec)

max_comps <- min(20L, length(pred_cols))
grid <- dials::grid_regular(
  dials::num_comp(range = c(2L, max(2L, max_comps))),
  dials::predictor_prop(range = c(0.5, 1.0)),
  levels = n_levels
)

tuned <- tune::tune_grid(
  wf,
  resamples = resamps,
  grid      = grid,
  metrics   = metric_set_cv,
  control   = tune::control_grid(save_pred = TRUE)
)

best <- tune::select_best(tuned, metric = "rmse")
final_wf  <- finalize_workflow(wf, best)
final_fit <- parsnip::fit(final_wf, data = train_dat)

# ----------------------- METRICS -----------------------------
# pooled CV metrics including rsq
cv_preds <- tune::collect_predictions(tuned) %>%
  dplyr::filter(num_comp == best$num_comp,
                abs(predictor_prop - best$predictor_prop) < 1e-12) %>%
  dplyr::mutate(truth = train_dat[[outcome]][.row])

cv_metrics <- metric_set_all(cv_preds, truth = truth, estimate = .pred) %>%
  dplyr::transmute(dataset = "cv(train)", .metric, estimate = .estimate)

# test metrics
test_preds <- predict(final_fit, new_data = test_dat) %>%
  dplyr::bind_cols(truth = test_dat[[outcome]])

test_metrics <- metric_set_all(test_preds, truth = truth, estimate = .pred) %>%
  dplyr::transmute(dataset = "test", .metric, estimate = .estimate)

summary_tbl <- dplyr::bind_rows(cv_metrics, test_metrics) %>%
  dplyr::arrange(match(dataset, c("cv(train)","test")), .metric)

print(summary_tbl)

# ----------------------- PLOTTING ----------------------------
pred_train <- predict(final_fit, new_data = train_dat) %>%
  dplyr::bind_cols(train_dat %>% dplyr::select(dplyr::all_of(c(id_col, time_col, outcome)))) %>%
  dplyr::mutate(set = "train")

pred_test  <- predict(final_fit, new_data = test_dat) %>%
  dplyr::bind_cols(test_dat %>% dplyr::select(dplyr::all_of(c(id_col, time_col, outcome)))) %>%
  dplyr::mutate(set = "test")

pred_all <- dplyr::bind_rows(pred_train, pred_test)

p <- # Real (black line)
  ggplot() +
  geom_line(data = df_mod,
            aes(x = .data[[time_col]], y = .data[[outcome]], color = "Real"),
            alpha = 0.6, linewidth = 1) +

  # Train predictions (red points)
  geom_point(data = pred_all %>% dplyr::filter(set == "train"),
             aes(x = .data[[time_col]], y = .pred, color = "Trained prediction"),
             alpha = 0.8, size = 1) +

  # Test predictions (green points)
  geom_point(data = pred_all %>% dplyr::filter(set == "test"),
             aes(x = .data[[time_col]], y = .pred, color = "Test prediction"),
             alpha = 0.9, size = 1.2) +

  facet_wrap(vars(.data[[id_col]]), scales = "free_x", nrow = 2) +
  labs(x = time_col, y = outcome, color = NULL) +
  scale_color_manual(
    values = c(
      "Real" = "black",
      "Trained prediction" = "red",
      "Test prediction" = "green4"
    )
  ) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "bottom",
    axis.title.x = element_blank(),
    axis.text.x = element_text(angle = 90, hjust = 0, vjust = 0.5,size = 9, colour = "black") ) +
  scale_x_date(labels = scales::date_format("%m-%d"),
               breaks = scales::date_breaks("1 week"),
               expand = c(0.01,0.01))

print(p)

# Real values plotted against predictions with linear regression lines added
rsq_tbl <- rsq(test_preds, truth = truth, estimate = .pred)
SSE_11 <- sum((test_preds$truth - test_preds$.pred)^2)
SST    <- sum((test_preds$truth - mean(test_preds$truth))^2)
r2_11  <- 1 - SSE_11 / SST
df_pred <- test_preds %>% rename(Real = truth, Prediction = .pred)

fmt <- function(x) formatC(x, digits = 2, format = "f")
lab_r2    <- paste0('R^2 == ', fmt(rsq_tbl$.estimate))
lab_r2_11 <- paste0('R["1:1"]^2 == ', fmt(r2_11))

xr <- range(df_pred$Prediction); yr <- range(df_pred$Real)
x_annot <- xr[1] + 0.60*diff(xr)
y_annot <- yr[1] + 0.38*diff(yr)
dy      <- 0.15*diff(yr)  # line spacing

p2 <- ggplot(df_pred, aes(Prediction, Real)) +
  geom_point(color = "black", size = 1.6) +
  geom_smooth(method = "lm", se = FALSE, color = "green4", linewidth = 1.1) +
  geom_abline(slope = 1, intercept = 0, color = "black") +
  annotate("text", x = x_annot, y = y_annot,
           label = lab_r2, parse = TRUE, hjust = 0, vjust = 1,
           size = 4.5, color = "green4") +
  annotate("text", x = x_annot, y = y_annot - dy,
           label = lab_r2_11, parse = TRUE, hjust = 0, vjust = 1,
           size = 4.5, color = "black") +
  # Digester label (top-left corner)
  annotate("text",
           x = xr[1] + 0.02 * diff(xr),
           y = yr[2] - 0.02 * diff(yr),
           label = "D1 and D2",
           hjust = 0, vjust = 1,
           size = 3, fontface = "bold") +
  labs(x = "Prediction", y = "Real")
  #theme_minimal(base_size = 12)
p2

blank <- ggplot() + theme_void()  # empty spacer
g1 <- ggpubr::ggarrange(
  p,
  ggpubr::ggarrange(
    blank, p2, blank,        # top spacer, main plot, bottom spacer
    ncol = 1,
    heights = c(1, 2, 1)     # adjust middle value (2) to control centering
  ),
  widths = c(0.6, 0.4)
)

ggsave(plot = g1, "./Figures/chronosplit_modeleval_alkalinity.png", width = 20, height = 12, units = "cm")
# Optional saves:
# readr::write_csv(summary_tbl, "summary_metrics_cv_test.csv")
# readr::write_csv(pred_all,    "predictions_train_test.csv")

```

